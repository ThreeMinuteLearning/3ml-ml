{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758129a8-e17f-4d77-b961-0b0d7b2e12f3",
   "metadata": {},
   "source": [
    "# TF-IDF for 3ml Stories\n",
    "\n",
    "We build a story recommender using TF-IDF and cosine similarity.\n",
    "\n",
    "The data is extracted from the 3ml database from within the `psql` repl using the command:\n",
    "\n",
    "```\n",
    "\\copy (select id,title,content from story) to stories.csv with csv delimiter ',' HEADER\n",
    "```\n",
    "\n",
    "or alternatively:\n",
    "\n",
    "```\n",
    "psql --csv -d my3ml -o stories.csv -c 'select id,title,content from story'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ddd5fd79-1190-4633-bbfc-388c04b245c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tekul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tekul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "46cb1eaa-8201-4b94-89b8-4eb42d785e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(document):\n",
    "    document = document.replace('&quot;', '')\n",
    "    document = document.replace('_', '')\n",
    "    tokens = word_tokenize(document)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2e78061f-a743-44cd-b29f-5fc9fae32254",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.read_csv('./stories.csv')\n",
    "# Skip short content\n",
    "stories['content'] = stories['content'].apply(clean_text)\n",
    "stories = stories[stories['content'].apply(lambda x: len(str(x)) > 30)]\n",
    "stories.reset_index(drop = True, inplace=True)\n",
    "#stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "58a928c3-a0cf-4c19-aee4-fbf543f1f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(stories.index, index=stories['id'])\n",
    "texts = stories['content']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "similarity = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e3adad4f-1550-4896-bd1b-abc8737a75aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4617"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('features.txt', 'w') as f:\n",
    "    for row in features:\n",
    "        f.write(str(row))\n",
    "        f.write('\\n')\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "342107ec-86b6-4fc7-b28b-ac3a878e65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "muscles = stories[stories['title'] == 'Making muscles move']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0e06082d-e023-4214-a53c-9064982010e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_stories(story):\n",
    "    id = story['id']\n",
    "    # Get the actual index in the stories dataframe from the id\n",
    "    index = indices.loc[id]\n",
    "    # Get the similarity scores for each story in the corpus\n",
    "    # enumerate gives us (index, value) tuples so we don't lose the position after sorting\n",
    "    scores = list(enumerate(similarity[index]))\n",
    "    # Sort and take the first ten closet matches\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    scores = scores[1:11]\n",
    "    # Get the indices of the closest matches and user those to get the ids\n",
    "    sim_indices = [s[0] for s in scores]\n",
    "    return list(stories['id'].iloc[sim_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "32bce6a2-5a89-439c-8c42-9df15b42184b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[968, 964, 963, 965, 985, 966, 967, 989, 988, 969]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_stories(stories.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "edd58d56-1f1f-4897-9dd6-bb9093017086",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories['nn'] = stories.apply(get_similar_stories, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a5503abe-ca6a-432d-9568-2dc8d41073c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stories.drop(columns=['content', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "44b1d6e3-673f-4868-96b2-99f11ba47617",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_json(path_or_buf=\"stories_nn.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f0abc-8a8b-4ade-b756-7025f7c8eebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
